{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlUZyP2xFCZx"
   },
   "source": [
    "various time series classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OsP1q2UuIdwo"
   },
   "outputs": [],
   "source": [
    "from sktime.datasets import load_UCR_UEA_dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(name):\n",
    "    X_train, y_train = load_UCR_UEA_dataset(name, split = 'train', return_type = 'numpy2d')\n",
    "    X_test, y_test = load_UCR_UEA_dataset(name, split = 'test', return_type = 'numpy2d')\n",
    "\n",
    "    labels_idx = {label: idx for idx, label in enumerate(np.unique(np.append(y_train, y_test)))}\n",
    "\n",
    "    y_train = np.array([labels_idx[y] for y in y_train])\n",
    "    y_test = np.array([labels_idx[y] for y in y_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "datasets = [get_datasets('ECG5000'), get_datasets('Car'), get_datasets('ECG5000')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "REPEAT_SIZE = 2\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "class LoggerCallback(keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            print('\\rFinished epoch ' + str(epoch), end='')\n",
    "\n",
    "def get_callbacks(validation=True):\n",
    "    return [LoggerCallback(), keras.callbacks.EarlyStopping(monitor = 'val_loss' if validation else 'loss', patience = 10, restore_best_weights = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 408us/step - loss: 0.2683 - accuracy: 0.9409\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 0.2548 - accuracy: 0.9316\n",
      "141/141 [==============================] - 0s 843us/step - loss: 0.2342 - accuracy: 0.9360\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 0.3187 - accuracy: 0.9178\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.9033 - accuracy: 0.7000\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 1.4065 - accuracy: 0.2167\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.8946 - accuracy: 0.6333\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.4123 - accuracy: 0.2500\n",
      "141/141 [==============================] - 0s 391us/step - loss: 0.2603 - accuracy: 0.9311\n",
      "141/141 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.9293\n",
      "141/141 [==============================] - 0s 800us/step - loss: 0.2336 - accuracy: 0.9400\n",
      "141/141 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.9238\n",
      "Fully connected average score 0.8573333223660787\n",
      "RNN average score 0.6925185223420461\n",
      "CNN average score 0.8364444375038147\n",
      "CNN GRU average score 0.6971851785977682\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_fully_connected(input_size, output_size):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape = [input_size]),\n",
    "        keras.layers.Dense(300, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(100, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(50, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(20, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "def build_rnn(input_size, output_size):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.SimpleRNN(64, input_shape = [input_size, 1], return_sequences = True),\n",
    "        keras.layers.SimpleRNN(64),\n",
    "        keras.layers.Dense(32, activation = 'relu'),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_cnn(input_size, output_size):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape = [input_size, 1]),\n",
    "        keras.layers.Conv1D(16, 3, activation = 'relu'),\n",
    "        keras.layers.MaxPool1D(2),\n",
    "        keras.layers.Conv1D(32, 3, activation = 'relu'),\n",
    "        keras.layers.MaxPool1D(2),\n",
    "        keras.layers.Conv1D(64, 3, activation = 'relu'),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation = 'relu'),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "def build_cnn_gru(input_size, output_size):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape = [input_size, 1]),\n",
    "        keras.layers.Conv1D(64, 3, activation = 'relu'),\n",
    "        keras.layers.MaxPool1D(2),\n",
    "        keras.layers.Conv1D(64, 3, activation = 'relu'),\n",
    "        keras.layers.MaxPool1D(2),\n",
    "        keras.layers.GRU(20),\n",
    "        keras.layers.Dense(256, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.25),\n",
    "        keras.layers.Dense(32, activation = 'relu'),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "def evaluate(X_train, y_train, X_test, y_test):\n",
    "    input_size, output_size = X_train.shape[1], np.unique(y_train).shape[0]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, shuffle = True)\n",
    "    X_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    X_valid = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    models = [\n",
    "        ('Fully connected', build_fully_connected),\n",
    "        ('RNN', build_rnn),\n",
    "        ('CNN', build_cnn),\n",
    "        ('CNN GRU', build_cnn_gru)\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for name, build_model in models:\n",
    "        model = build_model(input_size, output_size)\n",
    "        model.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "        model.fit(X_train, validation_data = X_valid, epochs = 100, verbose = 0, callbacks = get_callbacks())\n",
    "        score = model.evaluate(X_test, y_test)[1]\n",
    "        \n",
    "        scores[name] = score\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    scores.append(evaluate(dataset[0], dataset[1], dataset[2], dataset[3]))\n",
    "\n",
    "for model in scores[0].keys():\n",
    "    model_scores = [s[model] for s in scores]\n",
    "    print(model + ' average score ' + str(np.average(model_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 407us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 736us/step - loss: 0.2390 - accuracy: 0.9340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 8ms/step - loss: 1.4038 - accuracy: 0.3536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.8820 - accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 28ms/step - loss: 1.4037 - accuracy: 0.2167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 404us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 0s 784us/step - loss: 0.2168 - accuracy: 0.9404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 7ms/step - loss: 1.3002 - accuracy: 0.3542\n",
      "CNN Classifier scores [0.9266666666666666, 0.6833333333333333, 0.9362222222222222]\n",
      "MLP scores [0.9340000152587891, 0.699999988079071, 0.9404444694519043]\n",
      "FCN scores [0.3535555601119995, 0.21666666865348816, 0.35422220826148987]\n"
     ]
    }
   ],
   "source": [
    "from sktime.classification.deep_learning.cnn import CNNClassifier\n",
    "\n",
    "def build_cnn_classifier(input_size, output_size):\n",
    "    return CNNClassifier(callbacks = get_callbacks(validation = False), n_epochs = 100)\n",
    "\n",
    "def build_mlp(input_size, output_size):\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape = input_size),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(500, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "def build_fcn(input_size, output_size):\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape = [input_size, 1]),\n",
    "        keras.layers.Conv1D(128, 8),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Conv1D(256, 5),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.Conv1D(128, 3),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.ReLU(),\n",
    "        keras.layers.GlobalAveragePooling1D(),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "def evaluate2(X_train, y_train, X_test, y_test):\n",
    "    input_size, output_size = X_train.shape[1], np.unique(y_train).shape[0]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, shuffle = True)\n",
    "\n",
    "    models = [\n",
    "        ('CNN Classifier', build_cnn_classifier),\n",
    "        ('MLP', build_mlp),\n",
    "        ('FCN', build_fcn)\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    converted_to_tf = False\n",
    "\n",
    "    for name, build_model in models:\n",
    "\n",
    "        model = build_model(input_size, output_size)\n",
    "\n",
    "        if not converted_to_tf and (name == 'MLP' or name == 'FCN'):\n",
    "            X_train = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "            X_valid = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "            converted_to_tf = True\n",
    "        \n",
    "        if name == 'CNN Classifier':\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            score = model.score(X_test, y_test)\n",
    "        elif name == 'MLP':\n",
    "            model.compile(optimizer = keras.optimizers.Adadelta(learning_rate = 0.1, epsilon = 1e-8), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "            model.fit(X_train, validation_data = X_valid, epochs = 100, verbose = 0, callbacks = get_callbacks())\n",
    "            score = model.evaluate(X_test, y_test)[1]\n",
    "        elif name == 'FCN':\n",
    "            model.compile(optimizer = keras.optimizers.Adam(epsilon = 1e-8), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "            model.fit(X_train, validation_data = X_valid, epochs = 100, verbose = 0, callbacks = get_callbacks())\n",
    "            score = model.evaluate(X_test, y_test)[1]\n",
    "        \n",
    "        scores[name] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    scores.append(evaluate2(dataset[0], dataset[1], dataset[2], dataset[3]))\n",
    "\n",
    "for model in scores[0].keys():\n",
    "    model_scores = [s[model] for s in scores]\n",
    "    print(model + ' scores ' + str(model_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbqsQIicG_nb"
   },
   "source": [
    "\n",
    "- Bi-Direction LSTM and CNN networks separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "z00ZOCqDFZeC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 [==============================] - 1s 7ms/step - loss: 16.4934 - accuracy: 0.3533\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 5.4692 - accuracy: 0.5838\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 1.4301 - accuracy: 0.2000\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 1.3866 - accuracy: 0.2500\n",
      "141/141 [==============================] - 1s 6ms/step - loss: 3.7008 - accuracy: 0.3533\n",
      "141/141 [==============================] - 1s 4ms/step - loss: 1.5576 - accuracy: 0.5838\n",
      "FCN scores [0.35333332419395447, 0.20000000298023224, 0.35333332419395447]\n",
      "LSTM CNN scores [0.5837777853012085, 0.25, 0.5837777853012085]\n"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.series.detrend import Detrender\n",
    "from sktime.transformations.series.exponent import ExponentTransformer\n",
    "\n",
    "def build_lstm_cnn(input_size, output_size):\n",
    "    inputs = keras.layers.Input(shape = [input_size, 1])\n",
    "    lstm = keras.layers.Bidirectional(keras.layers.LSTM(30, return_sequences = True))(inputs)\n",
    "    cnn = keras.layers.Conv1D(60, 5, activation = 'relu', padding = 'same')(inputs)\n",
    "    concat = keras.layers.Concatenate()([lstm, cnn])\n",
    "    global_avg = keras.layers.GlobalAveragePooling1D()(concat)\n",
    "    dense = keras.layers.Dense(20, activation='relu')(global_avg)\n",
    "    softmax = keras.layers.Dense(output_size, activation = 'softmax')(dense)\n",
    "    model = keras.models.Model(inputs = inputs, outputs = softmax)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate3(X_train, y_train, X_test, y_test):\n",
    "    input_size, output_size = X_train.shape[1], np.unique(y_train).shape[0]\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, shuffle = True)\n",
    "\n",
    "    models = [\n",
    "        ('FCN', build_fcn),\n",
    "        ('LSTM CNN', build_lstm_cnn)\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for name, build_model in models:\n",
    "        model = build_model(input_size, output_size)\n",
    "        model.compile(optimizer = 'sgd', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "        if name == 'FCN':\n",
    "            trans_1 = ExponentTransformer(power=2)\n",
    "            trans_2 = Detrender()\n",
    "            pipe = trans_1*trans_2\n",
    "            X_train = pipe.fit_transform(X_train)\n",
    "            X_valid = pipe.transform(X_valid) \n",
    "            X_test = pipe.transform(X_test)\n",
    "\n",
    "        X_train_tf = tf.data.Dataset.from_tensor_slices((X_train, y_train)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "        X_valid_tf = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        model.fit(X_train_tf, validation_data = X_valid_tf, epochs = 100, verbose = 0, callbacks = get_callbacks())\n",
    "        score = model.evaluate(X_test, y_test)[1]\n",
    "        scores[name] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    scores.append(evaluate3(dataset[0], dataset[1], dataset[2], dataset[3]))\n",
    "\n",
    "for model in scores[0].keys():\n",
    "    model_scores = [s[model] for s in scores]\n",
    "    print(model + ' scores ' + str(model_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYOXtHBTFZ_E"
   },
   "source": [
    "Time series classification using sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "KccHsEFbFcPO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline score 0.9051111111111111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_split.py:737: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model SklearnClassifierPipeline(classifier=DecisionTreeClassifier(min_samples_leaf=10),\n",
      "                          transformers=[('exponent', ExponentTransformer()),\n",
      "                                        ('catch22',\n",
      "                                         Catch22(outlier_norm=True))])\n",
      "Best score 0.9046666666666666\n"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.panel.catch22 import Catch22\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sktime.classification.compose import SklearnClassifierPipeline\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = datasets[0]#ECG5000\n",
    "\n",
    "pipe = SklearnClassifierPipeline(DecisionTreeClassifier(), [('exponent', ExponentTransformer()), ('catch22', Catch22())])\n",
    "param_grid = {'classifier__min_samples_leaf': [1, 3, 5, 10], 'expoonent__power': [1, 2, 3], 'catch22__outlier_norm': [True, False]}\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print('Baseline score ' + str(pipe.score(X_test, y_test)))\n",
    "\n",
    "grid = RandomizedSearchCV(pipe, param_distributions=param_grid, n_iter=3)\n",
    "grid.fit(X_train, y_train)\n",
    "print('Best model ' + str(grid.best_estimator_))\n",
    "print('Best score ' + str(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0od-gD76TlmO"
   },
   "source": [
    "Multivariate time series classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "UVXhy9vEuDbK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 11ms/step - loss: 0.5341 - accuracy: 0.7971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 1s 83ms/step - loss: 1.5763 - accuracy: 0.2738\n",
      "MLP scores [0.7971014380455017, 0.2737642526626587]\n",
      "Mini Rocket scores [1.0, 0.4600760456273764]\n",
      "Rocket scores [0.9782608695652174, 0.4220532319391635]\n"
     ]
    }
   ],
   "source": [
    "from sktime.classification.kernel_based import RocketClassifier\n",
    "\n",
    "def get_datasets2(name):\n",
    "    X_train, y_train = load_UCR_UEA_dataset(name, split = 'train', return_type = 'numpy3d')\n",
    "    X_test, y_test = load_UCR_UEA_dataset(name, split = 'test', return_type = 'numpy3d')\n",
    "\n",
    "    labels_idx = {label: idx for idx, label in enumerate(np.unique(np.append(y_train, y_test)))}\n",
    "\n",
    "    y_train = np.array([labels_idx[y] for y in y_train])\n",
    "    y_test = np.array([labels_idx[y] for y in y_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "datasets2 = [get_datasets2('Epilepsy'), get_datasets2('EthanolConcentration')]\n",
    "\n",
    "def build_minirocket(input_shape, output_size):\n",
    "    return RocketClassifier(rocket_transform = 'minirocket')\n",
    "\n",
    "def build_rocket(input_shape, output_size):\n",
    "    return RocketClassifier()\n",
    "\n",
    "def build_multivariate_mlp(input_shape, output_size):\n",
    "    return keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape = input_shape),\n",
    "        keras.layers.Dropout(0.1),\n",
    "        keras.layers.Dense(500, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(500, activation = 'relu'),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(output_size, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "def evaluate4(X_train, y_train, X_test, y_test):\n",
    "    input_shape, output_size = (X_train.shape[2], X_train.shape[1]), np.unique(y_train).shape[0]\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, stratify = y_train, shuffle = True)\n",
    "\n",
    "    models = [\n",
    "        ('MLP', build_multivariate_mlp),\n",
    "        ('Mini Rocket', build_minirocket),\n",
    "        ('Rocket', build_rocket)\n",
    "    ]\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for name, build_model in models:\n",
    "        model = build_model(input_shape, output_size)\n",
    "\n",
    "        if name == 'MLP':\n",
    "            X_train_tf = tf.data.Dataset.from_tensor_slices((X_train.transpose((0, 2, 1)), y_train)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "            X_valid_tf = tf.data.Dataset.from_tensor_slices((X_valid.transpose((0, 2, 1)), y_valid)).repeat(REPEAT_SIZE).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "            model.compile(optimizer = keras.optimizers.Adadelta(learning_rate = 0.1, epsilon = 1e-8), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "            model.fit(X_train_tf, validation_data = X_valid_tf, epochs = 100, verbose = 0, callbacks = get_callbacks())\n",
    "            score = model.evaluate(X_test.transpose((0, 2, 1)), y_test)[1]\n",
    "        else:\n",
    "            model.fit(X_train, y_train)\n",
    "            score = model.score(X_test, y_test)\n",
    "\n",
    "        scores[name] = score\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = []\n",
    "\n",
    "for dataset in datasets2:\n",
    "    scores.append(evaluate4(dataset[0], dataset[1], dataset[2], dataset[3]))\n",
    "\n",
    "for model in scores[0].keys():\n",
    "    model_scores = [s[model] for s in scores]\n",
    "    print(model + ' scores ' + str(model_scores))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
